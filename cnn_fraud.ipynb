{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    enfj\n",
       "1    enfj\n",
       "2    enfj\n",
       "3    enfj\n",
       "4    enfj\n",
       "Name: MBTI, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df = pd.read_csv('crop128/all.csv')\n",
    "\n",
    "photo_df = pd.DataFrame(columns=['photo'], dtype= 'object')\n",
    "\n",
    "# put the photo data into a dataframe\n",
    "for i in range(len(mbti_df)):\n",
    "    image = transform(Image.open('crop128/' + mbti_df['filename'][i]))\n",
    "    photo_df.loc[i] = [image]\n",
    "\n",
    "mbti_df = mbti_df['MBTI']\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_df.iloc[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: MBTI, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = ['t', 'f']\n",
    "\n",
    "# if mbti_df['MBTI'] includes alphabet[0], then mbti_df['MBTI'] = 1, else 0\n",
    "mbti_df = mbti_df.apply(lambda x: 1 if alphabet[0] in x else 0)\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1551\n",
      "0    1493\n",
      "Name: MBTI, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print the statistics of the mbti_df\n",
    "print(mbti_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data includes the element of 2-dim tensor\n",
    "train_data = photo_df['photo'].values\n",
    "train_label = mbti_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 data 중 train의 비율\n",
    "train_ratio = 0.8\n",
    "\n",
    "train_idx = np.random.choice(len(train_data), int(len(train_data) * train_ratio), replace=False)\n",
    "test_idx = np.array(list(set(range(len(train_data))) - set(train_idx)))\n",
    "\n",
    "test_data = train_data[test_idx]\n",
    "test_label = train_label[test_idx]\n",
    "\n",
    "train_data = train_data[train_idx]\n",
    "train_label = train_label[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MBTI_Dataset(Dataset):\n",
    "    def __init__(self, train_label, train_data):\n",
    "        self.train_label = train_label\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mbti = self.train_label[idx]\n",
    "        photo = self.train_data[idx]\n",
    "\n",
    "        return mbti, photo\n",
    "\n",
    "# parameter 값은 이것을 변경해주세요\n",
    "in_ch1 = 1\n",
    "out_ch1 = 32\n",
    "ker1 = 2\n",
    "stride1 = 1\n",
    "pad1 = 0\n",
    "\n",
    "out_ch2 = 64\n",
    "ker2 = 2\n",
    "stride2 = 1\n",
    "pad2 = 0\n",
    "\n",
    "out_ch3 = 128\n",
    "ker3 = 2\n",
    "stride3 = 1\n",
    "pad3 = 0\n",
    "\n",
    "out_ch4 = 128\n",
    "ker4 = 2\n",
    "stride4 = 1\n",
    "pad4 = 0\n",
    "\n",
    "out_ch5 = 64\n",
    "ker5 = 2\n",
    "stride5 = 1\n",
    "pad5 = 0\n",
    "\n",
    "\n",
    "pool_size1 = 2\n",
    "pool_size2 = 2\n",
    "pool_size3 = 2\n",
    "pool_size4 = 2\n",
    "pool_size5 = 2\n",
    "\n",
    "\n",
    "out_feat1 = 120\n",
    "out_feat2 = 84\n",
    "out_feat3 = 1\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Net, self).__init__()\n",
    "        input_height, input_width = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_ch1, out_channels = out_ch1, kernel_size = ker1, stride = stride1, padding = pad1)\n",
    "        self.pool1 = nn.MaxPool2d(pool_size1, pool_size1)\n",
    "\n",
    "        output1_height, output1_width = (input_height - ker1 + 2 * pad1) / stride1 + 1, (input_width - ker1 + 2 * pad1) / stride1 + 1\n",
    "        output1_height, output1_width = int(output1_height / pool_size1), int(output1_width / pool_size1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_ch1, out_channels = out_ch2, kernel_size = ker2, stride = stride2, padding = pad2)\n",
    "        self.pool2 = nn.MaxPool2d(pool_size2, pool_size2)\n",
    "\n",
    "        output2_height, output2_width = (output1_height - ker2 + 2 * pad2) / stride2 + 1, (output1_width - ker2 + 2 * pad2) / stride2 + 1\n",
    "        output2_height, output2_width = int(output2_height / pool_size2), int(output2_width / pool_size2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = out_ch2, out_channels = out_ch3, kernel_size = ker3, stride = stride3, padding = pad3)\n",
    "        self.pool3 = nn.MaxPool2d(pool_size3, pool_size3)\n",
    "\n",
    "        output3_height, output3_width = (output2_height - ker3 + 2 * pad3) / stride3 + 1, (output2_width - ker3 + 2 * pad3) / stride3 + 1\n",
    "        output3_height, output3_width = int(output3_height / pool_size3), int(output3_width / pool_size3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels = out_ch3, out_channels = out_ch4, kernel_size = ker4, stride = stride4, padding = pad4)\n",
    "        self.pool4 = nn.MaxPool2d(pool_size4, pool_size4)\n",
    "\n",
    "        output4_height, output4_width = (output3_height - ker4 + 2 * pad4) / stride4 + 1, (output3_width - ker4 + 2 * pad4) / stride4 + 1\n",
    "        output4_height, output4_width = int(output4_height / pool_size4), int(output4_width / pool_size4)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels = out_ch4, out_channels = out_ch5, kernel_size = ker5, stride = stride5, padding = pad5)\n",
    "        self.pool5 = nn.MaxPool2d(pool_size5, pool_size5)\n",
    "\n",
    "        output5_height, output5_width = (output4_height - ker5 + 2 * pad5) / stride5 + 1, (output4_width - ker5 + 2 * pad5) / stride5 + 1\n",
    "        output5_height, output5_width = int(output5_height / pool_size5), int(output5_width / pool_size5)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_ch5 * output5_height * output5_width, out_feat1)\n",
    "        self.fc2 = nn.Linear(out_feat1, out_feat2)\n",
    "        self.fc3 = nn.Linear(out_feat2, out_feat3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print('1')\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        #print('2')\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        #print('3')\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        #print('4')\n",
    "        x = self.pool5(F.relu(self.conv5(x)))\n",
    "        #print('5')\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # 이 부분은 변경하셔도 괜찮아요. relu로 할지 sigmoid로 할지\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class cnn_model():\n",
    "    def __init__(self, model, lr=0.01, epochs=100, momentum = 0.6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum = self.momentum)\n",
    "    \n",
    "    def fit(self, X_train, y_train):        \n",
    "        self.trainloader = DataLoader(MBTI_Dataset(X_train, y_train), batch_size=64, shuffle=False)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for i, data in enumerate(self.trainloader):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                labels.unsqueeze_(1)\n",
    "                loss = self.criterion(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x.unsqueeze(0))\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'lr': self.lr, 'epochs': self.epochs, 'momentum': self.momentum}\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net((train_data[0].shape[1], train_data[0].shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_val_score(model, train_data, label, cv=5):\n",
    "    k = cv\n",
    "    kf = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "\n",
    "    acc_score = []\n",
    "    auc_score = []\n",
    "    \n",
    "    for train_index , test_index in kf.split(train_data):\n",
    "        X_train , X_test = train_data[train_index],train_data[test_index]\n",
    "        y_train , y_test = label[train_index] , label[test_index]\n",
    "        \n",
    "        if(np.unique(y_test).shape[0] == 1):\n",
    "            print('only one class')\n",
    "            continue\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        pred_values = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model.predict(X_test[i])\n",
    "            pred_values.append(pred.item())\n",
    "\n",
    "        auc = roc_auc_score(y_test, pred_values)\n",
    "        auc_score.append(auc)\n",
    "        \n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "    avg_auc_score = sum(auc_score)/k\n",
    "    \n",
    "    return avg_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimization for hyperparameter tuning\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = X\n",
    "    #train_data = lgb.Dataset(data=X, label=y, categorical_feature = cat_features, free_raw_data=False)\n",
    "    # define your metric\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "        params = {'application':'binary', 'num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 8.99),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (0.001, 0.1),\n",
    "                                            'min_child_weight': (5, 50)}, random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    # output optimization process\n",
    "    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n",
    "    # return best parameters\n",
    "    return lgbBO.max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tupes of (lr, momentum, epochs) randomly\n",
    "# random cv를 몇번 돌릴 것인지...\n",
    "random_cv_num = 30\n",
    "\n",
    "\n",
    "# parameter 값이 이 범위 내에서 나옵니다\n",
    "lrs = np.linspace(0.01, 0.06, 30)\n",
    "momentums = np.linspace(0.0, 0.9, 20)\n",
    "epochss = np.linspace(50, 200, 5, dtype=int)\n",
    "\n",
    "params = [(lr, momentum, epochs) for lr in lrs for momentum in momentums for epochs in epochss]\n",
    "np.random.shuffle(params)\n",
    "params = params[:random_cv_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved_accuracy: -2.9556650246305436\n",
      "learning rate: 0.041034482758620684 momentum: 0.8526315789473684 epochs: 50\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.0496551724137931 momentum: 0.8526315789473684 epochs: 87\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.041034482758620684 momentum: 0.09473684210526316 epochs: 162\n",
      "improved_accuracy: -1.1494252873563204\n",
      "learning rate: 0.027241379310344822 momentum: 0.09473684210526316 epochs: 200\n",
      "improved_accuracy: -0.6568144499178974\n",
      "learning rate: 0.027241379310344822 momentum: 0.14210526315789473 epochs: 87\n",
      "improved_accuracy: -0.9852216748768461\n",
      "learning rate: 0.030689655172413788 momentum: 0.2368421052631579 epochs: 50\n",
      "improved_accuracy: -0.8210180623973717\n",
      "learning rate: 0.011724137931034483 momentum: 0.3315789473684211 epochs: 50\n",
      "improved_accuracy: -0.6568144499178974\n",
      "learning rate: 0.03586206896551724 momentum: 0.9 epochs: 50\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.032413793103448274 momentum: 0.4736842105263158 epochs: 200\n",
      "improved_accuracy: -0.8210180623973717\n",
      "learning rate: 0.06 momentum: 0.09473684210526316 epochs: 162\n",
      "improved_accuracy: -0.49261083743842304\n",
      "learning rate: 0.06 momentum: 0.0 epochs: 50\n",
      "improved_accuracy: -0.49261083743842304\n",
      "learning rate: 0.046206896551724136 momentum: 0.4263157894736842 epochs: 162\n",
      "improved_accuracy: 0.49261083743842304\n",
      "learning rate: 0.032413793103448274 momentum: 0.2368421052631579 epochs: 200\n",
      "-------improved!--------\n",
      "best threshold:  -0.1720000000000912\n",
      "best accuracy:  0.5303776683087028\n",
      "best improved accuracy:  0.49261083743842304\n",
      "best auc:  0.5351481401384083\n",
      "best mean:  0.0009241547075758097\n",
      "best var:  8.856515234684215e-16\n",
      "best learning rate:  0.032413793103448274 best momentum:  0.2368421052631579 best epochs:  200\n",
      "------------------------\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.05655172413793103 momentum: 0.04736842105263158 epochs: 162\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.013448275862068966 momentum: 0.2368421052631579 epochs: 87\n",
      "improved_accuracy: 0.49261083743842304\n",
      "learning rate: 0.05137931034482758 momentum: 0.7578947368421053 epochs: 87\n",
      "improved_accuracy: 0.9852216748768461\n",
      "learning rate: 0.05655172413793103 momentum: 0.6157894736842106 epochs: 50\n",
      "-------improved!--------\n",
      "best threshold:  -0.08650000000010061\n",
      "best accuracy:  0.535303776683087\n",
      "best improved accuracy:  0.9852216748768461\n",
      "best auc:  0.5371485726643599\n",
      "best mean:  0.0007240380620673773\n",
      "best var:  5.930340178999516e-16\n",
      "best learning rate:  0.05655172413793103 best momentum:  0.6157894736842106 best epochs:  50\n",
      "------------------------\n",
      "improved_accuracy: 0.9852216748768461\n",
      "learning rate: 0.01 momentum: 0.4263157894736842 epochs: 87\n",
      "improved_accuracy: 0.8210180623973717\n",
      "learning rate: 0.013448275862068966 momentum: 0.04736842105263158 epochs: 162\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.03413793103448275 momentum: 0.8526315789473684 epochs: 87\n",
      "improved_accuracy: 0.9852216748768461\n",
      "learning rate: 0.0496551724137931 momentum: 0.8052631578947369 epochs: 200\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.05137931034482758 momentum: 0.6631578947368422 epochs: 125\n",
      "improved_accuracy: 0.6568144499178974\n",
      "learning rate: 0.030689655172413788 momentum: 0.9 epochs: 50\n",
      "improved_accuracy: 0.9852216748768461\n",
      "learning rate: 0.046206896551724136 momentum: 0.4263157894736842 epochs: 87\n",
      "improved_accuracy: 0.8210180623973717\n",
      "learning rate: 0.018620689655172412 momentum: 0.7578947368421053 epochs: 50\n",
      "improved_accuracy: 0.8210180623973717\n",
      "learning rate: 0.015172413793103447 momentum: 0.6631578947368422 epochs: 200\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.032413793103448274 momentum: 0.7578947368421053 epochs: 50\n",
      "improved_accuracy: 0.9852216748768461\n",
      "learning rate: 0.020344827586206895 momentum: 0.28421052631578947 epochs: 87\n",
      "improved_accuracy: 1.1494252873563204\n",
      "learning rate: 0.03758620689655172 momentum: 0.7105263157894737 epochs: 87\n",
      "-------improved!--------\n",
      "best threshold:  -0.1528000000000933\n",
      "best accuracy:  0.5369458128078818\n",
      "best improved accuracy:  1.1494252873563204\n",
      "best auc:  0.5426632785467128\n",
      "best mean:  0.0003833619827376889\n",
      "best var:  2.1175718552257672e-16\n",
      "best learning rate:  0.03758620689655172 best momentum:  0.7105263157894737 best epochs:  87\n",
      "------------------------\n",
      "improved_accuracy: 0.8210180623973717\n",
      "learning rate: 0.054827586206896546 momentum: 0.14210526315789473 epochs: 50\n",
      "-------------final result-------------\n",
      "best threshold:  0.5369458128078818\n",
      "best accuracy:  0.5369458128078818\n",
      "best improved accuracy:  1.1494252873563204\n",
      "best auc:  0.5426632785467128\n",
      "best_mean:  0.0003833619827376889\n",
      "best_var:  2.1175718552257672e-16\n",
      "best learning rate:  0.03758620689655172 best momentum:  0.7105263157894737 best epochs:  87\n"
     ]
    }
   ],
   "source": [
    "#import roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "models = [cnn_model(net, lr, epochs, momentum) for lr, momentum, epochs in params]\n",
    "\n",
    "overall_best_threshold = 0\n",
    "overall_best_accuracy = 0\n",
    "overall_best_improved_accuracy = 0\n",
    "overall_best_auc = 0\n",
    "overall_best_mean = 0\n",
    "overall_best_var = 0\n",
    "overall_best_params = None\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_data, train_label)\n",
    "\n",
    "    train_pred_values = []\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        pred = model.predict(train_data[i])\n",
    "        train_pred_values.append(pred.item())\n",
    "\n",
    "    #calculate mean and variance of train_pred\n",
    "    train_pred_values = np.array(train_pred_values)\n",
    "    train_pred_mean = np.mean(train_pred_values)\n",
    "    train_pred_var = np.var(train_pred_values)\n",
    "\n",
    "    #normalize train_pred_values\n",
    "    train_pred_values = (train_pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for threshold in np.arange(-1, 1, 0.0001):\n",
    "        y_pred = np.array(train_pred_values) > threshold\n",
    "        score = accuracy_score(train_label, y_pred)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "\n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = model.predict(test_data[i])\n",
    "        pred_values.append(pred.item())\n",
    "\n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #normalize pred_values\n",
    "    pred_values = (pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "    guess3 = roc_auc_score(test_label, pred_values)\n",
    "\n",
    "    pred_values[pred_values >= best_threshold] = 1\n",
    "    pred_values[pred_values < best_threshold] = 0\n",
    "\n",
    "    #calculate accuracy score\n",
    "\n",
    "    guess1 = accuracy_score(test_label, pred_values)\n",
    "\n",
    "    # random guess\n",
    "\n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = 0\n",
    "        pred_values.append(pred)\n",
    "\n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #calculate accuracy score\n",
    "    guess2_1 = accuracy_score(test_label, pred_values)\n",
    "    \n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = 1\n",
    "        pred_values.append(pred)\n",
    "    \n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #calculate accuracy score\n",
    "    guess2_2 = accuracy_score(test_label, pred_values)\n",
    "\n",
    "    guess2 = max(guess2_1, guess2_2)\n",
    "\n",
    "    improved_accuracy = (guess1 - guess2) * 100\n",
    "\n",
    "    print('improved_accuracy:', improved_accuracy)\n",
    "    print('learning rate:', model.get_params()['lr'], 'momentum:', model.get_params()['momentum'], 'epochs:', model.get_params()['epochs'])\n",
    "\n",
    "    if(overall_best_improved_accuracy < improved_accuracy):\n",
    "        overall_best_threshold = best_threshold\n",
    "        overall_best_accuracy = guess1\n",
    "        overall_best_improved_accuracy = improved_accuracy\n",
    "        overall_best_auc = guess3\n",
    "        overall_best_mean = train_pred_mean\n",
    "        overall_best_var = train_pred_var\n",
    "        overall_best_params = model.get_params()\n",
    "        best_model = model\n",
    "\n",
    "        print('-------improved!--------')\n",
    "        print('best threshold: ', best_threshold)\n",
    "        print('best accuracy: ', guess1)\n",
    "        print('best improved accuracy: ', improved_accuracy)\n",
    "        print('best auc: ', guess3)\n",
    "        print('best mean: ', train_pred_mean)\n",
    "        print('best var: ', train_pred_var)\n",
    "        print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "        print('------------------------')\n",
    "\n",
    "if(overall_best_params == None):\n",
    "    print('test failed!')\n",
    "else:\n",
    "    print('-------------final result-------------')\n",
    "    print('best threshold: ', overall_best_accuracy)\n",
    "    print('best accuracy: ', overall_best_accuracy)\n",
    "    print('best improved accuracy: ', overall_best_improved_accuracy)\n",
    "    print('best auc: ', overall_best_auc)   \n",
    "    print('best_mean: ', overall_best_mean)\n",
    "    print('best_var: ', overall_best_var)\n",
    "    print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "    best_model.save('cnn_sn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best parameters in pickle file\n",
    "import pickle\n",
    "\n",
    "with open('best_params.pickle', 'wb') as f:\n",
    "    pickle.dump(overall_best_params, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
