{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# put the photo data into a dataframe\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(mbti_df)):\n\u001b[0;32m----> 7\u001b[0m     image \u001b[39m=\u001b[39m transform(Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mcrop128/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m mbti_df[\u001b[39m'\u001b[39;49m\u001b[39mfilename\u001b[39;49m\u001b[39m'\u001b[39;49m][i]))\n\u001b[1;32m      8\u001b[0m     photo_df\u001b[39m.\u001b[39mloc[i] \u001b[39m=\u001b[39m [image]\n\u001b[1;32m     10\u001b[0m mbti_df \u001b[39m=\u001b[39m mbti_df[\u001b[39m'\u001b[39m\u001b[39mMBTI\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "mbti_df = pd.read_csv('crop128/all.csv')\n",
    "\n",
    "photo_df = pd.DataFrame(columns=['photo'], dtype= 'object')\n",
    "\n",
    "# put the photo data into a dataframe\n",
    "for i in range(len(mbti_df)):\n",
    "    image = transform(Image.open('crop128/' + mbti_df['filename'][i]))\n",
    "    photo_df.loc[i] = [image]\n",
    "\n",
    "mbti_df = mbti_df['MBTI']\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [14], line 12\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[39m# Run Bayesian Optimization\u001b[39;00m\n",
      "\u001b[1;32m     11\u001b[0m nn_bo \u001b[39m=\u001b[39m BayesianOptimization(nn_cl_bo, params_nn, random_state\u001b[39m=\u001b[39m\u001b[39m123\u001b[39m)\n",
      "\u001b[0;32m---> 12\u001b[0m nn_bo\u001b[39m.\u001b[39;49mmaximize(init_points\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(nn_bo\u001b[39m.\u001b[39mmax)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/bayes_opt/bayesian_optimization.py:311\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n",
      "\u001b[1;32m    309\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n",
      "\u001b[1;32m    310\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;32m--> 311\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobe(x_probe, lazy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m    314\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n",
      "\u001b[1;32m    315\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n",
      "\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n",
      "\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n",
      "\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n",
      "\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mprobe(params)\n",
      "\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n",
      "\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n",
      "\u001b[1;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n",
      "\u001b[0;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "\n",
      "Cell \u001b[0;32mIn [12], line 8\u001b[0m, in \u001b[0;36mnn_cl_bo\u001b[0;34m(neurons, activation, optimizer, learning_rate, batch_size, epochs)\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(neurons))\n",
      "\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;32m----> 8\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer(lr\u001b[39m=\u001b[39;49mlearning_rate), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m      9\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mfit(train_data, train_label, batch_size\u001b[39m=\u001b[39mbatch_size, epochs\u001b[39m=\u001b[39mepochs, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=123)\n",
    "nn_bo.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "print(nn_bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_df.iloc[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: MBTI, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = ['s', 'n']\n",
    "\n",
    "# if mbti_df['MBTI'] includes alphabet[0], then mbti_df['MBTI'] = 1, else 0\n",
    "mbti_df = mbti_df.apply(lambda x: 1 if alphabet[0] in x else 0)\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1343\n",
      "1    1205\n",
      "Name: MBTI, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print the statistics of the mbti_df\n",
    "print(mbti_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data includes the element of 2-dim tensor\n",
    "train_data = photo_df['photo'].values\n",
    "train_label = mbti_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 data 중 train의 비율\n",
    "train_ratio = 0.8\n",
    "\n",
    "train_idx = np.random.choice(len(train_data), int(len(train_data) * train_ratio), replace=False)\n",
    "test_idx = np.array(list(set(range(len(train_data))) - set(train_idx)))\n",
    "\n",
    "test_data = train_data[test_idx]\n",
    "test_label = train_label[test_idx]\n",
    "\n",
    "train_data = train_data[train_idx]\n",
    "train_label = train_label[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MBTI_Dataset(Dataset):\n",
    "    def __init__(self, train_label, train_data):\n",
    "        self.train_label = train_label\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mbti = self.train_label[idx]\n",
    "        photo = self.train_data[idx]\n",
    "\n",
    "        return mbti, photo\n",
    "\n",
    "# parameter 값은 이것을 변경해주세요\n",
    "in_ch1 = 1\n",
    "out_ch1 = 8\n",
    "ker1 = 6\n",
    "stride1 = 1\n",
    "pad1 = 0\n",
    "\n",
    "out_ch2 = 16\n",
    "ker2 = 4\n",
    "stride2 = 1\n",
    "pad2 = 0\n",
    "\n",
    "out_ch3 = 32\n",
    "ker3 = 2\n",
    "stride3 = 1\n",
    "pad3 = 0\n",
    "\n",
    "out_ch4 = 64\n",
    "ker4 = 2\n",
    "stride4 = 1\n",
    "pad4 = 0\n",
    "\n",
    "out_ch5 = 128\n",
    "ker5 = 2\n",
    "stride5 = 1\n",
    "pad5 = 0\n",
    "\n",
    "\n",
    "pool_size1 = 2\n",
    "pool_size2 = 2\n",
    "pool_size3 = 2\n",
    "pool_size4 = 2\n",
    "pool_size5 = 2\n",
    "\n",
    "\n",
    "out_feat1 = 128\n",
    "out_feat2 = 64\n",
    "out_feat3 = 16\n",
    "out_feat4 = 1\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Net, self).__init__()\n",
    "        input_height, input_width = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_ch1, out_channels = out_ch1, kernel_size = ker1, stride = stride1, padding = pad1)\n",
    "        self.pool1 = nn.MaxPool2d(pool_size1, pool_size1)\n",
    "\n",
    "        output1_height, output1_width = (input_height - ker1 + 2 * pad1) / stride1 + 1, (input_width - ker1 + 2 * pad1) / stride1 + 1\n",
    "        output1_height, output1_width = int(output1_height / pool_size1), int(output1_width / pool_size1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_ch1, out_channels = out_ch2, kernel_size = ker2, stride = stride2, padding = pad2)\n",
    "        self.pool2 = nn.MaxPool2d(pool_size2, pool_size2)\n",
    "\n",
    "        output2_height, output2_width = (output1_height - ker2 + 2 * pad2) / stride2 + 1, (output1_width - ker2 + 2 * pad2) / stride2 + 1\n",
    "        output2_height, output2_width = int(output2_height / pool_size2), int(output2_width / pool_size2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = out_ch2, out_channels = out_ch3, kernel_size = ker3, stride = stride3, padding = pad3)\n",
    "        self.pool3 = nn.MaxPool2d(pool_size3, pool_size3)\n",
    "\n",
    "        output3_height, output3_width = (output2_height - ker3 + 2 * pad3) / stride3 + 1, (output2_width - ker3 + 2 * pad3) / stride3 + 1\n",
    "        output3_height, output3_width = int(output3_height / pool_size3), int(output3_width / pool_size3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels = out_ch3, out_channels = out_ch4, kernel_size = ker4, stride = stride4, padding = pad4)\n",
    "        self.pool4 = nn.MaxPool2d(pool_size4, pool_size4)\n",
    "\n",
    "        output4_height, output4_width = (output3_height - ker4 + 2 * pad4) / stride4 + 1, (output3_width - ker4 + 2 * pad4) / stride4 + 1\n",
    "        output4_height, output4_width = int(output4_height / pool_size4), int(output4_width / pool_size4)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_ch4 * output4_height * output4_width, out_feat1)\n",
    "        self.fc2 = nn.Linear(out_feat1, out_feat2)\n",
    "        self.fc3 = nn.Linear(out_feat2, out_feat3)\n",
    "        self.fc4 = nn.Linear(out_feat3, out_feat4)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print('1')\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        #print('2')\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        #print('3')\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        #print('4')\n",
    "        #x = self.pool5(F.relu(self.conv5(x)))\n",
    "        #print('5')\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # 이 부분은 변경하셔도 괜찮아요. relu로 할지 sigmoid로 할지\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class cnn_model():\n",
    "    def __init__(self, model, lr=0.01, epochs=100, momentum = 0.6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum = self.momentum)\n",
    "    \n",
    "    def fit(self, X_train, y_train):        \n",
    "        self.trainloader = DataLoader(MBTI_Dataset(X_train, y_train), batch_size=64, shuffle=False)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for i, data in enumerate(self.trainloader):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                labels.unsqueeze_(1)\n",
    "                loss = self.criterion(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x.unsqueeze(0))\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'lr': self.lr, 'epochs': self.epochs, 'momentum': self.momentum}\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net((train_data[0].shape[1], train_data[0].shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_val_score(model, train_data, label, cv=5):\n",
    "    k = cv\n",
    "    kf = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "\n",
    "    acc_score = []\n",
    "    auc_score = []\n",
    "    \n",
    "    for train_index , test_index in kf.split(train_data):\n",
    "        X_train , X_test = train_data[train_index],train_data[test_index]\n",
    "        y_train , y_test = label[train_index] , label[test_index]\n",
    "        \n",
    "        if(np.unique(y_test).shape[0] == 1):\n",
    "            print('only one class')\n",
    "            continue\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        pred_values = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model.predict(X_test[i])\n",
    "            pred_values.append(pred.item())\n",
    "\n",
    "        auc = roc_auc_score(y_test, pred_values)\n",
    "        auc_score.append(auc)\n",
    "        \n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "    avg_auc_score = sum(auc_score)/k\n",
    "    \n",
    "    return avg_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tupes of (lr, momentum, epochs) randomly\n",
    "# random cv를 몇번 돌릴 것인지...\n",
    "random_num = 50\n",
    "\n",
    "\n",
    "# parameter 값이 이 범위 내에서 나옵니다\n",
    "lrs = np.linspace(0.01, 0.06, 30)\n",
    "momentums = np.linspace(0.0, 0.9, 20)\n",
    "epochss = np.linspace(150, 400, 25, dtype=int)\n",
    "\n",
    "params = [(lr, momentum, epochs) for lr in lrs for momentum in momentums for epochs in epochss]\n",
    "np.random.shuffle(params)\n",
    "params = params[:random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved_accuracy: -4.509803921568622\n",
      "learning rate: 0.022068965517241378 momentum: 0.2368421052631579 epochs: 212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m best_model \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m---> 17\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train_data, train_label)\n\u001b[0;32m     19\u001b[0m     train_pred_values \u001b[39m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_data)):\n",
      "Cell \u001b[1;32mIn [22], line 140\u001b[0m, in \u001b[0;36mcnn_model.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m    138\u001b[0m labels\u001b[39m.\u001b[39munsqueeze_(\u001b[39m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m--> 140\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "model = cnn_model(net, lr=0.01, epochs=50, momentum = 0.6)\n",
    "\n",
    "train_pred_values = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    pred = model.predict(train_data[i])\n",
    "    train_pred_values.append(pred.item())\n",
    "\n",
    "#calculate mean and variance of train_pred\n",
    "train_pred_values = np.array(train_pred_values)\n",
    "train_pred_mean = np.mean(train_pred_values)\n",
    "train_pred_var = np.var(train_pred_values)\n",
    "\n",
    "#normalize train_pred_values\n",
    "train_pred_values = (train_pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "\n",
    "best_threshold = 0\n",
    "best_score = 0\n",
    "\n",
    "for threshold in np.arange(-1, 1, 0.0001):\n",
    "    y_pred = np.array(train_pred_values) > threshold\n",
    "    score = accuracy_score(train_label, y_pred)\n",
    "    if score > best_score:\n",
    "        best_threshold = threshold\n",
    "        best_score = score\n",
    "\n",
    "pred_values = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    pred = model.predict(test_data[i])\n",
    "    pred_values.append(pred.item())\n",
    "\n",
    "#quantize predictions\n",
    "pred_values = np.array(pred_values)\n",
    "\n",
    "#normalize pred_values\n",
    "pred_values = (pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "\n",
    "print(best_threshold)\n",
    "print(train_pred_mean)\n",
    "print(train_pred_var)\n",
    "\n",
    "model.save('cnn_sn.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------final result-------------\n",
      "best threshold:  0.5303776683087028\n",
      "best accuracy:  0.5303776683087028\n",
      "best improved accuracy:  0.6568144499178974\n",
      "best auc:  0.5101070154577884\n",
      "best_mean:  0.0003726313971853155\n",
      "best_var:  4.695636660473933e-16\n",
      "best learning rate:  0.01689655172413793 best momentum:  0.28421052631578947 best epochs:  191\n"
     ]
    }
   ],
   "source": [
    "if(overall_best_params == None):\n",
    "    print('test failed!')\n",
    "else:\n",
    "    print('-------------final result-------------')\n",
    "    print('best threshold: ', overall_best_threshold)\n",
    "    print('best accuracy: ', overall_best_accuracy)\n",
    "    print('best improved accuracy: ', overall_best_improved_accuracy)\n",
    "    print('best auc: ', overall_best_auc)   \n",
    "    print('best_mean: ', overall_best_mean)\n",
    "    print('best_var: ', overall_best_var)\n",
    "    print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "    best_model.save('cnn_sn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
